{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GET http://d2d-zephir-stg.cdlib.org/api/item/txu.059173018498868.json\n",
      "GET http://d2d-zephir-stg.cdlib.org/api/item/gri.ark:/13960/t91875m5j.json\n",
      "GET http://d2d-zephir-stg.cdlib.org/api/item/mdp.39015022391018.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'marc_embeddings.zephir.ZephirRecord'>\n"
     ]
    }
   ],
   "source": [
    "# Start by loading some data.\n",
    "\n",
    "from marc_embeddings.zephir import load_from_api\n",
    "\n",
    "marc_data = [\n",
    "    zephir.load_from_api('txu.059173018498868'),\n",
    "    zephir.load_from_api('gri.ark:/13960/t91875m5j'),\n",
    "    zephir.load_from_api('mdp.39015022391018')\n",
    "]\n",
    "\n",
    "print(type(marc_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>245</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>txu.059173018498868</th>\n",
       "      <td>The aftermath of sovereignty: West Indian pers...</td>\n",
       "      <td>Lowenthal, David.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gri.ark:/13960/t91875m5j</th>\n",
       "      <td>As sepulturas do Espinheiro / por Anselmo Braa...</td>\n",
       "      <td>Braamcamp Freire, Anselmo, 1849-1921.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mdp.39015022391018</th>\n",
       "      <td>Chemical information systems, edited by Janet ...</td>\n",
       "      <td>Ash, Janet E.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        245  \\\n",
       "txu.059173018498868       The aftermath of sovereignty: West Indian pers...   \n",
       "gri.ark:/13960/t91875m5j  As sepulturas do Espinheiro / por Anselmo Braa...   \n",
       "mdp.39015022391018        Chemical information systems, edited by Janet ...   \n",
       "\n",
       "                                                            100  \n",
       "txu.059173018498868                           Lowenthal, David.  \n",
       "gri.ark:/13960/t91875m5j  Braamcamp Freire, Anselmo, 1849-1921.  \n",
       "mdp.39015022391018                                Ash, Janet E.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output as a pandas dataframe.\n",
    "\n",
    "from marc_embeddings.zephir import ZephirTransformer\n",
    "\n",
    "dataframe = ZephirTransformer(['245', '100'], dataframe = True).transform(marc_data)\n",
    "print(type(dataframe))\n",
    "display(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The aftermath of sovereignty: West Indian perspectives. Edited and introduced by David Lowenthal and Lambros Comitas.', 'Lowenthal, David.'], ['As sepulturas do Espinheiro / por Anselmo Braamcamp Freire.', 'Braamcamp Freire, Anselmo, 1849-1921.'], ['Chemical information systems, edited by Janet E. Ash [and] Ernest Hyde.', 'Ash, Janet E.']]\n"
     ]
    }
   ],
   "source": [
    "# Output as a simple list of list of strings.\n",
    "\n",
    "data = ZephirTransformer(['245', '100']).transform(marc_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The aftermath of sovereignty: West Indian perspectives. Edited and introduced by David Lowenthal and Lambros Comitas.'], ['As sepulturas do Espinheiro / por Anselmo Braamcamp Freire.'], ['Chemical information systems, edited by Janet E. Ash [and] Ernest Hyde.']]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a9d81bb05531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# not a list of lists of string documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Count the words that occur in the 245 field.\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = ZephirTransformer(['245']).transform(marc_data)\n",
    "\n",
    "# This is a list of list of Strings\n",
    "print(data)\n",
    "\n",
    "# This won't work, because CountVectorizer expects a list of string documents,\n",
    "# not a list of lists of string documents.\n",
    "try:\n",
    "    counts = CountVectorizer().fit_transform(data)\n",
    "except:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from marc_embeddings.zephir import FlattenTransformer\n",
    "\n",
    "try:\n",
    "    counts = CountVectorizer().fit_transform(FlattenTransformer().transform(data))\n",
    "    print(type(counts))\n",
    "except:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30)\n",
      "[[1 2 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1]\n",
      " [0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "# particularly the \"Advantages of the CSR format\" and \"Disadvantages of the CSR format\" sections.\n",
    "\n",
    "print(counts.shape)\n",
    "print(counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3, 30)\n",
      "[[ 0.25424316  0.38671695  0.          0.          0.          0.\n",
      "   0.19335847  0.          0.25424316  0.25424316  0.          0.19335847\n",
      "   0.          0.          0.          0.          0.25424316  0.\n",
      "   0.25424316  0.          0.25424316  0.25424316  0.25424316  0.25424316\n",
      "   0.          0.          0.25424316  0.          0.25424316  0.25424316]\n",
      " [ 0.          0.          0.35355339  0.35355339  0.          0.35355339\n",
      "   0.          0.          0.          0.          0.35355339  0.          0.\n",
      "   0.35355339  0.35355339  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.35355339  0.35355339\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.25732238  0.          0.          0.338348    0.\n",
      "   0.25732238  0.338348    0.          0.          0.          0.25732238\n",
      "   0.338348    0.          0.          0.338348    0.          0.338348    0.\n",
      "   0.338348    0.          0.          0.          0.          0.          0.\n",
      "   0.          0.338348    0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Finally, why not normalize these word counts?\n",
    "\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "normalized = TfidfTransformer().fit_transform(counts)\n",
    "print(type(normalized))\n",
    "print(normalized.shape)\n",
    "print(normalized.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Extract the MARC 245 data.', ZephirTransformer(dataframe=None, selection=['245'])), ('Remove extra [].', FlattenTransformer()), ('Count distinct words.', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='conte...cument frequency.', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Count distinct words.': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'Extract the MARC 245 data.': ZephirTransformer(dataframe=None, selection=['245']),\n",
       " 'Normalize for word frequency, inverse document frequency.': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'Remove extra [].': FlattenTransformer()}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All of these steps, involving a 'fit' or 'transform' can be combined into a single object.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('Extract the MARC 245 data.', ZephirTransformer(['245'])),\n",
    "    ('Remove extra [].', FlattenTransformer()),\n",
    "    ('Count distinct words.', CountVectorizer()),\n",
    "    ('Normalize for word frequency, inverse document frequency.', TfidfTransformer())\n",
    "])\n",
    "\n",
    "display(pipeline)\n",
    "display(pipeline.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.25424316,  0.38671695,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.19335847,  0.        ,  0.25424316,  0.25424316,\n",
       "          0.        ,  0.19335847,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.25424316,  0.        ,  0.25424316,  0.        ,\n",
       "          0.25424316,  0.25424316,  0.25424316,  0.25424316,  0.        ,\n",
       "          0.        ,  0.25424316,  0.        ,  0.25424316,  0.25424316],\n",
       "        [ 0.        ,  0.        ,  0.35355339,  0.35355339,  0.        ,\n",
       "          0.35355339,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.35355339,  0.        ,  0.        ,  0.35355339,  0.35355339,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.35355339,\n",
       "          0.35355339,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.25732238,  0.        ,  0.        ,  0.338348  ,\n",
       "          0.        ,  0.25732238,  0.338348  ,  0.        ,  0.        ,\n",
       "          0.        ,  0.25732238,  0.338348  ,  0.        ,  0.        ,\n",
       "          0.338348  ,  0.        ,  0.338348  ,  0.        ,  0.338348  ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.338348  ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(marc_data)\n",
    "pipeline.transform(marc_data).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next step, read Scikit's tutorial on pipelines and parameter optimization.\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
